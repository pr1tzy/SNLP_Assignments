{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19dd58ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "- data_csv: data/IMDB Dataset.csv\n",
      "- output_dir: outputs\n",
      "- models: ['distilbert-base-uncased', 'bert-base-uncased', 'roberta-base', 'google/electra-small-discriminator', 'albert-base-v2']\n",
      "- subset_size: 8000\n",
      "- epochs_subset: 1, epochs_full: 3\n",
      "- batch_size: 16, max_length: 256\n",
      "- seed: 42\n",
      "\n",
      "=== Subset fine-tuning: distilbert-base-uncased ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train (distilbert-base-uncased): 100%|██████████| 8000/8000 [00:01<00:00, 6096.43 examples/s]\n",
      "Tokenizing val (distilbert-base-uncased): 100%|██████████| 5000/5000 [00:00<00:00, 6195.06 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_43950/1140283204.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.225300</td>\n",
       "      <td>0.271427</td>\n",
       "      <td>0.890678</td>\n",
       "      <td>0.891400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: distilbert-base-uncased -> F1: 0.8907, Acc: 0.8914, saved at: outputs/subset_distilbert-base-uncased\n",
      "\n",
      "=== Subset fine-tuning: bert-base-uncased ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train (bert-base-uncased): 100%|██████████| 8000/8000 [00:01<00:00, 5597.96 examples/s]\n",
      "Tokenizing val (bert-base-uncased): 100%|██████████| 5000/5000 [00:00<00:00, 5857.68 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_43950/1140283204.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>0.257187</td>\n",
       "      <td>0.897446</td>\n",
       "      <td>0.898800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: bert-base-uncased -> F1: 0.8974, Acc: 0.8988, saved at: outputs/subset_bert-base-uncased\n",
      "\n",
      "=== Subset fine-tuning: roberta-base ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train (roberta-base): 100%|██████████| 8000/8000 [00:01<00:00, 6908.26 examples/s]\n",
      "Tokenizing val (roberta-base): 100%|██████████| 5000/5000 [00:00<00:00, 6924.45 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_43950/1140283204.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.229602</td>\n",
       "      <td>0.922467</td>\n",
       "      <td>0.921800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: roberta-base -> F1: 0.9225, Acc: 0.9218, saved at: outputs/subset_roberta-base\n",
      "\n",
      "=== Subset fine-tuning: google/electra-small-discriminator ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train (google/electra-small-discriminator): 100%|██████████| 8000/8000 [00:01<00:00, 5925.72 examples/s]\n",
      "Tokenizing val (google/electra-small-discriminator): 100%|██████████| 5000/5000 [00:00<00:00, 5936.94 examples/s]\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_43950/1140283204.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.359500</td>\n",
       "      <td>0.371767</td>\n",
       "      <td>0.867659</td>\n",
       "      <td>0.864800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: google/electra-small-discriminator -> F1: 0.8677, Acc: 0.8648, saved at: outputs/subset_google_electra-small-discriminator\n",
      "\n",
      "=== Subset fine-tuning: albert-base-v2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train (albert-base-v2): 100%|██████████| 8000/8000 [00:01<00:00, 5129.79 examples/s]\n",
      "Tokenizing val (albert-base-v2): 100%|██████████| 5000/5000 [00:00<00:00, 5128.91 examples/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_43950/1140283204.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.216000</td>\n",
       "      <td>0.250282</td>\n",
       "      <td>0.906741</td>\n",
       "      <td>0.906200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: albert-base-v2 -> F1: 0.9067, Acc: 0.9062, saved at: outputs/subset_albert-base-v2\n",
      "\n",
      "=== Best model on subset ===\n",
      "roberta-base with F1: 0.9225, Acc: 0.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing full train (roberta-base): 100%|██████████| 40000/40000 [00:05<00:00, 6685.34 examples/s]\n",
      "Tokenizing full val (roberta-base): 100%|██████████| 5000/5000 [00:00<00:00, 6979.68 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_43950/1140283204.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 19:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>0.219551</td>\n",
       "      <td>0.933964</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.224442</td>\n",
       "      <td>0.937937</td>\n",
       "      <td>0.937800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>0.291123</td>\n",
       "      <td>0.938939</td>\n",
       "      <td>0.939000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final best model eval on val] roberta-base -> F1: 0.9389, Acc: 0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Inference on 10 random test samples ===\n",
      "[01] Pred: NEGATIVE  (score=0.997) | Gold: NEGATIVE\n",
      "     A plane carrying a rich scientist's daughter goes down in thick wilderness. He assembles a group to go and find her and the others, but the rescue party soon suspects that something is stalking them. Then ulterior motive...\n",
      "\n",
      "[02] Pred: NEGATIVE  (score=0.999) | Gold: NEGATIVE\n",
      "     If any movie ever made Italians look bad, this is it.<br /><br />Duke Mitchell - what an A--HOLE. Duke Mitchell, I s--t on your grave. Seeing as practically every person gunned down in this film by the cowardly Mimi is e...\n",
      "\n",
      "[03] Pred: POSITIVE  (score=0.998) | Gold: NEGATIVE\n",
      "     I desperately want to give this movie a 10...I really do. Some movies, especially horror movies are so budget that they are good. A wise-cracking ninja scarecrow who can implement corn cobs as lethal weaponry...definitel...\n",
      "\n",
      "[04] Pred: NEGATIVE  (score=1.000) | Gold: NEGATIVE\n",
      "     It seems that several of the people who have reviewed this movie only watched it in the first place because it was filmed near where they live. How's that for a ringing endorsement? If this movie was filmed near where I ...\n",
      "\n",
      "[05] Pred: POSITIVE  (score=0.995) | Gold: POSITIVE\n",
      "     The basic plot of this film has already been detailed in several other comments so I won't bother. I'd like to first commend the producer, cast, directors and crew for creating a wonderfully engaging film on a meager, $1...\n",
      "\n",
      "[06] Pred: NEGATIVE  (score=1.000) | Gold: NEGATIVE\n",
      "     OK i will admit, it started out very pleasing and good, but then it just dropped downhill, i cannot believe Sarah Michelle Gellar could have even finished reading the script after about 5 minutes into the movie, the only...\n",
      "\n",
      "[07] Pred: NEGATIVE  (score=1.000) | Gold: NEGATIVE\n",
      "     Have I seen a worse movie? No I can't say that I have. This was pathetic. If the director is still alive: 1. He shouldn't be. 2. He should be ashamed. 3. God, how I would like to take out my 2 completely wasted hours of ...\n",
      "\n",
      "[08] Pred: NEGATIVE  (score=0.999) | Gold: NEGATIVE\n",
      "     There is a lot of obvious hype associated with this film. Let's just face it, though, the main reasons why anyone would watch it would be for Leo and Cate, who are not necessarily the best actors in this film. I'm not sa...\n",
      "\n",
      "[09] Pred: NEGATIVE  (score=1.000) | Gold: NEGATIVE\n",
      "     well its official. they have just killed American Pie. The first 3 were absolutely hysterical, but this one and the others have been awful. I mean the story is about two college fraternity's who battle each other for its...\n",
      "\n",
      "[10] Pred: POSITIVE  (score=0.996) | Gold: POSITIVE\n",
      "     Due to reading bad reviews and being told by friends that they couldn't believe how bad it was, I didn't go and see this film at the cinema. After watching it on DVD, I have to say I regret that now. I'm not saying it is...\n",
      "\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Configuration defaults\n",
    "# -----------------------\n",
    "DEFAULT_DATA_CSV = \"data/IMDB Dataset.csv\"\n",
    "DEFAULT_OUTPUT_DIR = \"outputs\"\n",
    "\n",
    "# 5 candidate models to try on a subset\n",
    "CANDIDATE_MODEL_NAMES = [\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"google/electra-small-discriminator\",\n",
    "    \"albert-base-v2\",\n",
    "]\n",
    "\n",
    "ID2LABEL = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "LABEL2ID = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "\n",
    "def load_and_prepare_df(csv_path: str, seed: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load the Kaggle IMDB CSV and split into train/val/test.\n",
    "    Kaggle columns: \"review\", \"sentiment\" (values 'positive'/'negative').\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if \"review\" not in df.columns or \"sentiment\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'review' and 'sentiment' columns.\")\n",
    "\n",
    "    # Convert labels to ints: positive -> 1, negative -> 0\n",
    "    df[\"label\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0}).astype(int)\n",
    "    df = df[[\"review\", \"label\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "    # Train/val/test split: 80/10/10\n",
    "    train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=seed, stratify=df[\"label\"])\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=0.1111, random_state=seed, stratify=train_val_df[\"label\"])  # 0.1111 of 90% ~ 10%\n",
    "\n",
    "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def make_tokenize_fn(tokenizer: AutoTokenizer, max_length: int):\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"review\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "    return tokenize\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelRunResult:\n",
    "    model_name: str\n",
    "    eval_f1: float\n",
    "    eval_accuracy: float\n",
    "    output_dir: str\n",
    "\n",
    "\n",
    "def compute_metrics_builder() -> callable:\n",
    "    # Custom F1 compute function for binary classification\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        f1 = f1_score(labels, preds, average=\"binary\")\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"f1\": f1, \"accuracy\": acc}\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "def build_trainer(\n",
    "    model_name_or_path: str,\n",
    "    num_labels: int,\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    output_dir: str,\n",
    "    learning_rate: float,\n",
    "    epochs: int,\n",
    "    per_device_train_batch_size: int,\n",
    "    per_device_eval_batch_size: int,\n",
    "    weight_decay: float,\n",
    "    warmup_ratio: float,\n",
    "    fp16: bool,\n",
    "    seed: int,\n",
    "    logging_steps: int = 50,\n",
    "    eval_strategy: str = \"epoch\",\n",
    "    save_strategy: str = \"epoch\",\n",
    "    metric_for_best: str = \"f1\",\n",
    "    max_length: int = 256,\n",
    ") -> Tuple[Trainer, AutoTokenizer, AutoModelForSequenceClassification]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label=ID2LABEL,\n",
    "        label2id=LABEL2ID,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    compute_metrics = compute_metrics_builder()\n",
    "\n",
    "    # Important: label names for Trainer\n",
    "    train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in [\"input_ids\", \"attention_mask\", \"label\"]])\n",
    "    val_ds = val_ds.remove_columns([c for c in val_ds.column_names if c not in [\"input_ids\", \"attention_mask\", \"label\"]])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=eval_strategy,\n",
    "        save_strategy=save_strategy,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        logging_steps=logging_steps,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=metric_for_best,\n",
    "        greater_is_better=True,\n",
    "        fp16=fp16,\n",
    "        dataloader_num_workers=2,\n",
    "        seed=seed,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    return trainer, tokenizer, model\n",
    "\n",
    "\n",
    "def finetune_and_evaluate(\n",
    "    model_name: str,\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    subset_size: int,\n",
    "    seed: int,\n",
    "    base_output_dir: str,\n",
    "    learning_rate: float,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    max_length: int,\n",
    "    fp16: bool,\n",
    ") -> ModelRunResult:\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    if subset_size and subset_size < len(train_df):\n",
    "        train_df_sub, _ = train_test_split(\n",
    "            train_df,\n",
    "            train_size=subset_size,\n",
    "            stratify=train_df[\"label\"],\n",
    "            random_state=seed,\n",
    "        )\n",
    "    else:\n",
    "        train_df_sub = train_df\n",
    "\n",
    "\n",
    "    # Make datasets\n",
    "    tmp_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    tokenize = make_tokenize_fn(tmp_tokenizer, max_length)\n",
    "    train_ds = Dataset.from_pandas(train_df_sub)\n",
    "    val_ds = Dataset.from_pandas(val_df)\n",
    "    train_ds = train_ds.map(tokenize, batched=True, desc=f\"Tokenizing train ({model_name})\")\n",
    "    val_ds = val_ds.map(tokenize, batched=True, desc=f\"Tokenizing val ({model_name})\")\n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    out_dir = os.path.join(base_output_dir, f\"subset_{model_name.replace('/', '_')}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    trainer, _, _ = build_trainer(\n",
    "        model_name_or_path=model_name,\n",
    "        num_labels=2,\n",
    "        train_ds=train_ds,\n",
    "        val_ds=val_ds,\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.06,\n",
    "        fp16=fp16,\n",
    "        seed=seed,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_metrics = trainer.evaluate()\n",
    "\n",
    "    # Free memory\n",
    "    trainer.save_state()\n",
    "    trainer.save_model(out_dir)\n",
    "\n",
    "    model_f1 = float(eval_metrics.get(\"eval_f1\", 0.0))\n",
    "    model_acc = float(eval_metrics.get(\"eval_accuracy\", 0.0))\n",
    "\n",
    "    # Clear CUDA cache between models\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return ModelRunResult(\n",
    "        model_name=model_name,\n",
    "        eval_f1=model_f1,\n",
    "        eval_accuracy=model_acc,\n",
    "        output_dir=out_dir,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_best_on_full(\n",
    "    best_model_name: str,\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    seed: int,\n",
    "    base_output_dir: str,\n",
    "    learning_rate: float,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    max_length: int,\n",
    "    fp16: bool,\n",
    ") -> Tuple[str, AutoTokenizer]:\n",
    "    # Rebuild tokenizer for full data\n",
    "    tokenizer = AutoTokenizer.from_pretrained(best_model_name, use_fast=True)\n",
    "    tokenize = make_tokenize_fn(tokenizer, max_length)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    val_ds = Dataset.from_pandas(val_df)\n",
    "\n",
    "    train_ds = train_ds.map(tokenize, batched=True, desc=f\"Tokenizing full train ({best_model_name})\")\n",
    "    val_ds = val_ds.map(tokenize, batched=True, desc=f\"Tokenizing full val ({best_model_name})\")\n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    out_dir = os.path.join(base_output_dir, f\"full_{best_model_name.replace('/', '_')}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    trainer, _, _ = build_trainer(\n",
    "        model_name_or_path=best_model_name,\n",
    "        num_labels=2,\n",
    "        train_ds=train_ds,\n",
    "        val_ds=val_ds,\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.06,\n",
    "        fp16=fp16,\n",
    "        seed=seed,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    final_metrics = trainer.evaluate()\n",
    "    print(f\"[Final best model eval on val] {best_model_name} -> F1: {final_metrics.get('eval_f1'):.4f}, Acc: {final_metrics.get('eval_accuracy'):.4f}\")\n",
    "\n",
    "    trainer.save_state()\n",
    "    trainer.save_model(out_dir)\n",
    "    return out_dir, tokenizer\n",
    "\n",
    "\n",
    "def run_inference(\n",
    "    model_dir: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    test_df: pd.DataFrame,\n",
    "    seed: int,\n",
    "    num_samples: int = 10,\n",
    "):\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "    clf = pipeline(\n",
    "        task=\"text-classification\",\n",
    "        model=model_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=32,\n",
    "        device=device,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "    samples = test_df.sample(n=min(num_samples, len(test_df)), random_state=seed)\n",
    "    texts: List[str] = samples[\"review\"].tolist()\n",
    "    true_labels: List[int] = samples[\"label\"].tolist()\n",
    "    preds = clf(texts)\n",
    "\n",
    "    print(\"\\n=== Inference on 10 random test samples ===\")\n",
    "    for i, (txt, gold, pred) in enumerate(zip(texts, true_labels, preds), 1):\n",
    "        gold_lbl = ID2LABEL[gold]\n",
    "        pred_lbl = pred[\"label\"]\n",
    "        score = pred.get(\"score\", None)\n",
    "        # Truncate review for display\n",
    "        short_txt = (txt[:220] + \"...\") if len(txt) > 220 else txt\n",
    "        if score is not None:\n",
    "            print(f\"[{i:02d}] Pred: {pred_lbl:>8}  (score={score:.3f}) | Gold: {gold_lbl:>8}\\n     {short_txt}\\n\")\n",
    "        else:\n",
    "            print(f\"[{i:02d}] Pred: {pred_lbl:>8} | Gold: {gold_lbl:>8}\\n     {short_txt}\\n\")\n",
    "\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"IMDB Sentiment Fine-tuning (5 models subset + best on full)\"\n",
    "    )\n",
    "    parser.add_argument(\"--data_csv\", type=str, default=DEFAULT_DATA_CSV)\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=DEFAULT_OUTPUT_DIR)\n",
    "    parser.add_argument(\"--subset_size\", type=int, default=8000)\n",
    "    parser.add_argument(\"--epochs_subset\", type=int, default=1)\n",
    "    parser.add_argument(\"--epochs_full\", type=int, default=3)\n",
    "    parser.add_argument(\"--learning_rate_subset\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--learning_rate_full\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    parser.add_argument(\"--max_length\", type=int, default=256)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--models\", type=str, nargs=\"*\", default=CANDIDATE_MODEL_NAMES)\n",
    "\n",
    "    # 👇 ignores the extra junk Jupyter passes\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Config:\")\n",
    "    print(f\"- data_csv: {args.data_csv}\")\n",
    "    print(f\"- output_dir: {args.output_dir}\")\n",
    "    print(f\"- models: {args.models}\")\n",
    "    print(f\"- subset_size: {args.subset_size}\")\n",
    "    print(f\"- epochs_subset: {args.epochs_subset}, epochs_full: {args.epochs_full}\")\n",
    "    print(f\"- batch_size: {args.batch_size}, max_length: {args.max_length}\")\n",
    "    print(f\"- seed: {args.seed}\")\n",
    "\n",
    "    # Reproducibility\n",
    "    set_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(args.seed)\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    # Load data\n",
    "    train_df, val_df, test_df = load_and_prepare_df(args.data_csv, seed=args.seed)\n",
    "\n",
    "    # Decide fp16 if GPU available\n",
    "    fp16 = torch.cuda.is_available()\n",
    "\n",
    "    # 1) Fine-tune 5 different models on subset and evaluate\n",
    "    results: List[ModelRunResult] = []\n",
    "    for model_name in args.models:\n",
    "        print(f\"\\n=== Subset fine-tuning: {model_name} ===\")\n",
    "        res = finetune_and_evaluate(\n",
    "            model_name=model_name,\n",
    "            train_df=train_df,\n",
    "            val_df=val_df,\n",
    "            subset_size=args.subset_size,\n",
    "            seed=args.seed,\n",
    "            base_output_dir=args.output_dir,\n",
    "            learning_rate=args.learning_rate_subset,\n",
    "            epochs=args.epochs_subset,\n",
    "            batch_size=args.batch_size,\n",
    "            max_length=args.max_length,\n",
    "            fp16=fp16,\n",
    "        )\n",
    "        print(f\"Result: {model_name} -> F1: {res.eval_f1:.4f}, Acc: {res.eval_accuracy:.4f}, saved at: {res.output_dir}\")\n",
    "        results.append(res)\n",
    "\n",
    "    # 2) Pick best by F1\n",
    "    results_sorted = sorted(results, key=lambda r: r.eval_f1, reverse=True)\n",
    "    best = results_sorted[0]\n",
    "    print(\"\\n=== Best model on subset ===\")\n",
    "    print(f\"{best.model_name} with F1: {best.eval_f1:.4f}, Acc: {best.eval_accuracy:.4f}\")\n",
    "\n",
    "    # 3) Fine-tune best model on full training set\n",
    "    best_model_dir, best_tokenizer = train_best_on_full(\n",
    "        best_model_name=best.model_name,\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        seed=args.seed,\n",
    "        base_output_dir=args.output_dir,\n",
    "        learning_rate=args.learning_rate_full,\n",
    "        epochs=args.epochs_full,\n",
    "        batch_size=args.batch_size,\n",
    "        max_length=args.max_length,\n",
    "        fp16=fp16,\n",
    "    )\n",
    "\n",
    "    # 4) Inference on 10 random samples from test set\n",
    "    run_inference(\n",
    "        model_dir=best_model_dir,\n",
    "        tokenizer=best_tokenizer,\n",
    "        test_df=test_df,\n",
    "        seed=args.seed,\n",
    "        num_samples=10,\n",
    "    )\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b1e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
