{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9541f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "- Resolved CSV: data/resolved_queries.csv\n",
      "- New CSV:      data/new_queries.csv\n",
      "- Base names:   data/base_names.csv\n",
      "- Variations:   data/name_variations.csv\n",
      "- Output dir:   outputs_matching\n",
      "- Top-k:        3\n",
      "\n",
      "=== Task 1: Matching new queries to resolved queries ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RapidFuzz:token_set_ratio: 100%|██████████| 20/20 [00:00<00:00, 6605.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saved RapidFuzz matches (token_set_ratio) -> outputs_matching/task1/task1_fuzzy_token_set_ratio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RapidFuzz:token_sort_ratio: 100%|██████████| 20/20 [00:00<00:00, 41262.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saved RapidFuzz matches (token_sort_ratio) -> outputs_matching/task1/task1_fuzzy_token_sort_ratio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RapidFuzz:partial_ratio: 100%|██████████| 20/20 [00:00<00:00, 14508.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saved RapidFuzz matches (partial_ratio) -> outputs_matching/task1/task1_fuzzy_partial_ratio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RapidFuzz:ratio: 100%|██████████| 20/20 [00:00<00:00, 128266.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saved RapidFuzz matches (ratio) -> outputs_matching/task1/task1_fuzzy_ratio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RapidFuzz:WRatio: 100%|██████████| 20/20 [00:00<00:00, 33447.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saved RapidFuzz matches (WRatio) -> outputs_matching/task1/task1_fuzzy_WRatio.csv\n",
      "\n",
      "RapidFuzz method statistics (Task 1):\n",
      "  token_set_ratio  median_top1=75.2 median_top2=41.7 median_gap=33.9\n",
      "  token_sort_ratio median_top1=70.5 median_top2=40.6 median_gap=29.4\n",
      "  partial_ratio    median_top1=66.7 median_top2=46.4 median_gap=20.3\n",
      "  ratio            median_top1=70.9 median_top2=41.1 median_gap=30.2\n",
      "  WRatio           median_top1=76.9 median_top2=42.5 median_gap=30.7\n",
      "=> Recommended RapidFuzz method: token_set_ratio with threshold ~= 84.1 (0-100 scale)\n",
      "- Saved recommended RF accepted matches -> outputs_matching/task1/task1_fuzzy_token_set_ratio_accepted.csv\n",
      "- Saved TF-IDF matches -> outputs_matching/task1/task1_tfidf_char.csv\n",
      "=> Recommended TF-IDF cosine threshold ~= 0.81 (0-1 scale)\n",
      "- Saved recommended TF-IDF accepted matches -> outputs_matching/task1/task1_tfidf_char_accepted.csv\n",
      "\n",
      "Notes:\n",
      "- For query matching, token_set_ratio is often robust to reordering and extra tokens.\n",
      "- Char-level TF-IDF (3-5) tends to work well for short queries with typos/variants.\n",
      "- The suggested thresholds are heuristic; adjust after spot-checking some pairs.\n",
      "\n",
      "=== Task 2: Matching names with variations ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RapidFuzz:token_set_ratio: 100%|██████████| 100/100 [00:00<00:00, 68111.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saved RapidFuzz name matches (token_set_ratio) -> outputs_matching/task2/task2_fuzzy_token_set_ratio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RapidFuzz:token_sort_ratio: 100%|██████████| 100/100 [00:00<00:00, 209296.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saved RapidFuzz name matches (token_sort_ratio) -> outputs_matching/task2/task2_fuzzy_token_sort_ratio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RapidFuzz:partial_ratio: 100%|██████████| 100/100 [00:00<00:00, 74778.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saved RapidFuzz name matches (partial_ratio) -> outputs_matching/task2/task2_fuzzy_partial_ratio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RapidFuzz:ratio: 100%|██████████| 100/100 [00:00<00:00, 350694.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saved RapidFuzz name matches (ratio) -> outputs_matching/task2/task2_fuzzy_ratio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RapidFuzz:WRatio: 100%|██████████| 100/100 [00:00<00:00, 49246.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Saved RapidFuzz name matches (WRatio) -> outputs_matching/task2/task2_fuzzy_WRatio.csv\n",
      "\n",
      "RapidFuzz method statistics (Task 2):\n",
      "  token_set_ratio  median_top1=100.0 median_top2=45.5 median_gap=53.8\n",
      "  token_sort_ratio median_top1=100.0 median_top2=45.5 median_gap=53.8\n",
      "  partial_ratio    median_top1=100.0 median_top2=53.0 median_gap=41.2\n",
      "  ratio            median_top1=100.0 median_top2=45.5 median_gap=53.8\n",
      "  WRatio           median_top1=100.0 median_top2=45.5 median_gap=52.4\n",
      "=> Recommended RapidFuzz method (names): token_set_ratio with threshold ~= 100.0 (0-100 scale)\n",
      "- Saved recommended RF accepted name matches -> outputs_matching/task2/task2_fuzzy_token_set_ratio_accepted.csv\n",
      "- Saved TF-IDF name matches -> outputs_matching/task2/task2_tfidf_char.csv\n",
      "=> Recommended TF-IDF cosine threshold (names) ~= 1.00 (0-1 scale)\n",
      "- Saved recommended TF-IDF accepted name matches -> outputs_matching/task2/task2_tfidf_char_accepted.csv\n",
      "\n",
      "Notes:\n",
      "- For names, token_set_ratio on token-sorted normalized names is typically best.\n",
      "- Char TF-IDF with (2,4) n-grams works well for initials and short tokens.\n",
      "- Consider post-filters: same first letter, matching last token, or length ratio within 0.6-1.6 for higher precision.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "# RapidFuzz is preferred over fuzzywuzzy for speed and licensing\n",
    "from rapidfuzz import fuzz, process\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Default data locations\n",
    "# ---------------------------\n",
    "DEFAULT_RESOLVED_QUERIES = \"data/resolved_queries.csv\"\n",
    "DEFAULT_NEW_QUERIES = \"data/new_queries.csv\"\n",
    "\n",
    "DEFAULT_BASE_NAMES = \"data/base_names.csv\"\n",
    "DEFAULT_NAME_VARIATIONS = \"data/name_variations.csv\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers: Text normalization\n",
    "# ---------------------------\n",
    "WS_RE = re.compile(r\"\\s+\")\n",
    "PUNCT_RE = re.compile(r\"[^\\w\\s]\")  # keep alphanumerics and underscore\n",
    "COMMA_SPLIT_RE = re.compile(r\"\\s*,\\s*\")\n",
    "\n",
    "HONORIFICS = {\n",
    "    \"mr\", \"mrs\", \"ms\", \"miss\", \"dr\", \"prof\", \"sir\", \"madam\", \"mx\",\n",
    "    \"jr\", \"sr\", \"ii\", \"iii\", \"iv\", \"v\"\n",
    "}\n",
    "\n",
    "def normalize_text_generic(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generic normalization for queries:\n",
    "    - ASCII fold\n",
    "    - Lowercase\n",
    "    - Collapse whitespace\n",
    "    - Strip\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "    t = unidecode(text).lower().strip()\n",
    "    t = WS_RE.sub(\" \", t)\n",
    "    return t\n",
    "\n",
    "\n",
    "def normalize_text_alnum(text: str) -> str:\n",
    "    \"\"\"\n",
    "    For fuzzy methods that benefit from punctuation removal.\n",
    "    \"\"\"\n",
    "    t = normalize_text_generic(text)\n",
    "    t = PUNCT_RE.sub(\" \", t)\n",
    "    t = WS_RE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def normalize_name(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Name-specific normalization:\n",
    "    - ASCII fold, lowercase\n",
    "    - Remove honorifics and suffixes\n",
    "    - Handle \"Last, First Middle\" -> \"First Middle Last\"\n",
    "    - Remove punctuation (retain spaces)\n",
    "    - Normalize initials: \"J. R. R.\" -> \"j r r\"\n",
    "    - Collapse multiple spaces\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "    t = unidecode(text).lower().strip()\n",
    "\n",
    "    # If contains a comma, treat as \"last, first ...\"\n",
    "    if \",\" in t:\n",
    "        parts = [p.strip() for p in COMMA_SPLIT_RE.split(t) if p.strip()]\n",
    "        if len(parts) >= 2:\n",
    "            # Move last name to the end\n",
    "            t = \" \".join(parts[1:] + [parts[0]])\n",
    "\n",
    "    # Remove punctuation\n",
    "    t = PUNCT_RE.sub(\" \", t)\n",
    "\n",
    "    # Remove honorifics/suffixes\n",
    "    tokens = [tok for tok in t.split() if tok not in HONORIFICS]\n",
    "    # Normalize initials by removing periods already handled; keep as single letters\n",
    "    t = \" \".join(tokens)\n",
    "    t = WS_RE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def sorted_token_form(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Token sort canonicalization (useful for token_set/token_sort ratios).\n",
    "    \"\"\"\n",
    "    toks = [tok for tok in text.split() if tok]\n",
    "    toks.sort()\n",
    "    return \" \".join(toks)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# RapidFuzz matching utilities\n",
    "# ---------------------------\n",
    "RAPIDFUZZ_SCORERS = {\n",
    "    \"ratio\": fuzz.ratio,\n",
    "    \"partial_ratio\": fuzz.partial_ratio,\n",
    "    \"token_sort_ratio\": fuzz.token_sort_ratio,\n",
    "    \"token_set_ratio\": fuzz.token_set_ratio,\n",
    "    \"WRatio\": fuzz.WRatio,\n",
    "}\n",
    "\n",
    "def rapidfuzz_topk(\n",
    "    queries: List[str],\n",
    "    candidates: List[str],\n",
    "    method: str,\n",
    "    topk: int = 3,\n",
    "    score_cutoff: int = 0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute top-k RapidFuzz matches for each query against candidates with a given scorer.\n",
    "\n",
    "    Returns:\n",
    "      indices: shape (len(queries), topk) indices into candidates (-1 if missing)\n",
    "      scores:  shape (len(queries), topk) similarity scores (0..100)\n",
    "    \"\"\"\n",
    "    scorer = RAPIDFUZZ_SCORERS[method]\n",
    "    n = len(queries)\n",
    "    idx_mat = np.full((n, topk), -1, dtype=int)\n",
    "    score_mat = np.zeros((n, topk), dtype=float)\n",
    "\n",
    "    for i, q in enumerate(tqdm(queries, desc=f\"RapidFuzz:{method}\")):\n",
    "        # process.extract returns list of (candidate, score, index)\n",
    "        matches = process.extract(q, candidates, scorer=scorer, limit=topk, score_cutoff=score_cutoff)\n",
    "        for j, tup in enumerate(matches):\n",
    "            # tup can be (string, score, index)\n",
    "            cand_idx = tup[2]\n",
    "            cand_score = tup[1]\n",
    "            idx_mat[i, j] = cand_idx\n",
    "            score_mat[i, j] = cand_score\n",
    "    return idx_mat, score_mat\n",
    "\n",
    "\n",
    "def suggest_threshold_from_gaps(top1: np.ndarray, top2: np.ndarray, hard_max: float, gap_margin: float) -> float:\n",
    "    \"\"\"\n",
    "    Heuristic threshold suggestion without ground truth:\n",
    "      - Use 95th percentile of top2 as a proxy for strong-but-incorrect matches\n",
    "      - Use 60th percentile of top1 as a proxy for typical true matches\n",
    "      - Add a small margin\n",
    "    Final threshold = max(q95(top2) + margin, q60(top1))\n",
    "    \"\"\"\n",
    "    if len(top1) == 0:\n",
    "        return 0.0\n",
    "    q95_top2 = np.nanpercentile(top2, 95)\n",
    "    q60_top1 = np.nanpercentile(top1, 60)\n",
    "    thr = max(q95_top2 + gap_margin, q60_top1)\n",
    "    thr = max(0.0, min(hard_max, thr))\n",
    "    return float(thr)\n",
    "\n",
    "\n",
    "def method_recommendation(score_mat: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute statistics to help method/threshold selection:\n",
    "    - median top1 score\n",
    "    - median top2 score\n",
    "    - median confidence gap (top1 - top2)\n",
    "    \"\"\"\n",
    "    top1 = score_mat[:, 0]\n",
    "    top2 = score_mat[:, 1] if score_mat.shape[1] > 1 else np.zeros_like(top1)\n",
    "    med_top1 = np.nanmedian(top1)\n",
    "    med_top2 = np.nanmedian(top2)\n",
    "    med_gap = np.nanmedian(top1 - top2)\n",
    "    return {\n",
    "        \"median_top1\": float(med_top1),\n",
    "        \"median_top2\": float(med_top2),\n",
    "        \"median_gap\": float(med_gap),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# TF-IDF + cosine utilities\n",
    "# ---------------------------\n",
    "def fit_tfidf_vectorizer(\n",
    "    corpus_ref: List[str],\n",
    "    analyzer: str = \"char_wb\",\n",
    "    ngram_range: Tuple[int, int] = (3, 5),\n",
    "    min_df: int = 1,\n",
    "    max_df: float = 1.0,\n",
    ") -> TfidfVectorizer:\n",
    "    vec = TfidfVectorizer(\n",
    "        analyzer=analyzer,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        lowercase=False,  # we already lowercased in normalization\n",
    "        norm=\"l2\",\n",
    "    )\n",
    "    vec.fit(corpus_ref)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def tfidf_topk(\n",
    "    vec: TfidfVectorizer,\n",
    "    queries: List[str],\n",
    "    candidates: List[str],\n",
    "    topk: int = 3,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Use NearestNeighbors on TF-IDF vectors to get top-k cosine-based matches.\n",
    "    Returns:\n",
    "      indices shape (n_queries, topk)\n",
    "      sims    shape (n_queries, topk), in [0,1]\n",
    "    \"\"\"\n",
    "    X_cand = vec.transform(candidates)\n",
    "    X_query = vec.transform(queries)\n",
    "\n",
    "    # Nearest neighbors on cosine distance, n_neighbors=topk\n",
    "    nn = NearestNeighbors(n_neighbors=min(topk, X_cand.shape[0]), metric=\"cosine\", algorithm=\"brute\")\n",
    "    nn.fit(X_cand)\n",
    "    distances, indices = nn.kneighbors(X_query)\n",
    "    # Convert cosine distance to cosine similarity\n",
    "    sims = 1.0 - distances\n",
    "    return indices, sims\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Data loading helpers\n",
    "# ---------------------------\n",
    "def load_csv_guess_text_column(path: str, preferred_cols: List[str]) -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Load CSV and guess which column holds the text to match.\n",
    "    preferred_cols: in order of preference, we'll take the first that exists.\n",
    "    Fallback: first object dtype column.\n",
    "    Returns: (df, chosen_column_name)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for c in preferred_cols:\n",
    "        if c.lower() in cols_lower:\n",
    "            col = cols_lower[c.lower()]\n",
    "            return df, col\n",
    "    # fallback to first string-like column\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            return df, c\n",
    "    # if none found, force-cast first column\n",
    "    return df, df.columns[0]\n",
    "\n",
    "\n",
    "def ensure_id_column(df: pd.DataFrame, base_name: str = \"id\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure an 'id' column exists for output. If not, create a sequential one.\n",
    "    \"\"\"\n",
    "    cols_lower = [c.lower() for c in df.columns]\n",
    "    if base_name in cols_lower:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"id\"] = np.arange(len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipelines\n",
    "# ---------------------------\n",
    "def task1_match_queries(\n",
    "    resolved_csv: str,\n",
    "    new_csv: str,\n",
    "    out_dir: str,\n",
    "    topk: int = 3,\n",
    ") -> None:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(\"\\n=== Task 1: Matching new queries to resolved queries ===\")\n",
    "\n",
    "    # Load data\n",
    "    resolved_df, resolved_col = load_csv_guess_text_column(resolved_csv, preferred_cols=[\"query\", \"resolved_query\", \"text\", \"question\"])\n",
    "    new_df, new_col = load_csv_guess_text_column(new_csv, preferred_cols=[\"query\", \"new_query\", \"text\", \"question\"])\n",
    "    resolved_df = ensure_id_column(resolved_df, base_name=\"id\")\n",
    "    new_df = ensure_id_column(new_df, base_name=\"id\")\n",
    "\n",
    "    # Normalize text\n",
    "    resolved_df[\"_norm\"] = resolved_df[resolved_col].map(normalize_text_alnum)\n",
    "    new_df[\"_norm\"] = new_df[new_col].map(normalize_text_alnum)\n",
    "\n",
    "    resolved_texts = resolved_df[\"_norm\"].fillna(\"\").tolist()\n",
    "    new_texts = new_df[\"_norm\"].fillna(\"\").tolist()\n",
    "\n",
    "    # A. RapidFuzz with multiple scorers\n",
    "    rf_results = {}\n",
    "    rf_stats = {}\n",
    "    for method in [\"token_set_ratio\", \"token_sort_ratio\", \"partial_ratio\", \"ratio\", \"WRatio\"]:\n",
    "        idx_mat, score_mat = rapidfuzz_topk(new_texts, resolved_texts, method=method, topk=topk)\n",
    "        rf_results[method] = (idx_mat, score_mat)\n",
    "        rf_stats[method] = method_recommendation(score_mat)\n",
    "\n",
    "        # Save detailed CSV\n",
    "        rows = []\n",
    "        for i in range(len(new_df)):\n",
    "            for j in range(topk):\n",
    "                ridx = idx_mat[i, j]\n",
    "                score = score_mat[i, j]\n",
    "                if ridx < 0:\n",
    "                    continue\n",
    "                rows.append({\n",
    "                    \"new_id\": new_df.iloc[i][\"id\"],\n",
    "                    \"new_query\": new_df.iloc[i][new_col],\n",
    "                    \"resolved_id\": resolved_df.iloc[ridx][\"id\"],\n",
    "                    \"resolved_query\": resolved_df.iloc[ridx][resolved_col],\n",
    "                    \"method\": method,\n",
    "                    \"rank\": j + 1,\n",
    "                    \"score\": score,\n",
    "                })\n",
    "        out_path = os.path.join(out_dir, f\"task1_fuzzy_{method}.csv\")\n",
    "        pd.DataFrame(rows).to_csv(out_path, index=False)\n",
    "        print(f\"- Saved RapidFuzz matches ({method}) -> {out_path}\")\n",
    "\n",
    "    # Suggest best method based on median confidence gap\n",
    "    method_scores = {m: rf_stats[m][\"median_gap\"] for m in rf_stats}\n",
    "    best_rf_method = max(method_scores, key=method_scores.get)\n",
    "    best_idx, best_score = rf_results[best_rf_method]\n",
    "    best_top1 = best_score[:, 0]\n",
    "    best_top2 = best_score[:, 1] if best_score.shape[1] > 1 else np.zeros_like(best_top1)\n",
    "    rf_threshold = suggest_threshold_from_gaps(best_top1, best_top2, hard_max=100.0, gap_margin=5.0)\n",
    "\n",
    "    print(\"\\nRapidFuzz method statistics (Task 1):\")\n",
    "    for m in rf_stats:\n",
    "        s = rf_stats[m]\n",
    "        print(f\"  {m:16s} median_top1={s['median_top1']:.1f} median_top2={s['median_top2']:.1f} median_gap={s['median_gap']:.1f}\")\n",
    "    print(f\"=> Recommended RapidFuzz method: {best_rf_method} with threshold ~= {rf_threshold:.1f} (0-100 scale)\")\n",
    "\n",
    "    # Produce a consolidated \"accepted\" CSV for the recommended RF method\n",
    "    accepted_rows = []\n",
    "    for i in range(len(new_df)):\n",
    "        ridx = best_idx[i, 0]\n",
    "        s1 = best_score[i, 0]\n",
    "        s2 = best_score[i, 1] if best_score.shape[1] > 1 else 0.0\n",
    "        accepted_rows.append({\n",
    "            \"new_id\": new_df.iloc[i][\"id\"],\n",
    "            \"new_query\": new_df.iloc[i][new_col],\n",
    "            \"resolved_id\": resolved_df.iloc[ridx][\"id\"] if ridx >= 0 else None,\n",
    "            \"resolved_query\": resolved_df.iloc[ridx][resolved_col] if ridx >= 0 else None,\n",
    "            \"method\": best_rf_method,\n",
    "            \"score_top1\": s1,\n",
    "            \"score_top2\": s2,\n",
    "            \"confidence_gap\": s1 - s2,\n",
    "            \"accepted_match\": bool(s1 >= rf_threshold),\n",
    "        })\n",
    "    rf_accept_path = os.path.join(out_dir, f\"task1_fuzzy_{best_rf_method}_accepted.csv\")\n",
    "    pd.DataFrame(accepted_rows).to_csv(rf_accept_path, index=False)\n",
    "    print(f\"- Saved recommended RF accepted matches -> {rf_accept_path}\")\n",
    "\n",
    "    # B. TF-IDF + cosine (char n-grams are robust for short queries)\n",
    "    vec = fit_tfidf_vectorizer(\n",
    "        corpus_ref=resolved_texts,\n",
    "        analyzer=\"char_wb\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=1,\n",
    "        max_df=1.0,\n",
    "    )\n",
    "    tf_idx, tf_sims = tfidf_topk(vec, new_texts, resolved_texts, topk=topk)\n",
    "    # Save detailed CSV\n",
    "    rows = []\n",
    "    for i in range(len(new_df)):\n",
    "        for j in range(tf_idx.shape[1]):\n",
    "            ridx = tf_idx[i, j]\n",
    "            sim = tf_sims[i, j]\n",
    "            rows.append({\n",
    "                \"new_id\": new_df.iloc[i][\"id\"],\n",
    "                \"new_query\": new_df.iloc[i][new_col],\n",
    "                \"resolved_id\": resolved_df.iloc[ridx][\"id\"],\n",
    "                \"resolved_query\": resolved_df.iloc[ridx][resolved_col],\n",
    "                \"method\": \"tfidf_char_wb_3_5\",\n",
    "                \"rank\": j + 1,\n",
    "                \"cosine_similarity\": sim,\n",
    "            })\n",
    "    tfidf_out = os.path.join(out_dir, \"task1_tfidf_char.csv\")\n",
    "    pd.DataFrame(rows).to_csv(tfidf_out, index=False)\n",
    "    print(f\"- Saved TF-IDF matches -> {tfidf_out}\")\n",
    "\n",
    "    # Threshold suggestion for TF-IDF\n",
    "    tf_top1 = tf_sims[:, 0]\n",
    "    tf_top2 = tf_sims[:, 1] if tf_sims.shape[1] > 1 else np.zeros_like(tf_top1)\n",
    "    tf_threshold = suggest_threshold_from_gaps(tf_top1, tf_top2, hard_max=1.0, gap_margin=0.05)\n",
    "\n",
    "    tf_accepted_rows = []\n",
    "    for i in range(len(new_df)):\n",
    "        ridx = tf_idx[i, 0]\n",
    "        s1 = tf_sims[i, 0]\n",
    "        s2 = tf_sims[i, 1] if tf_sims.shape[1] > 1 else 0.0\n",
    "        tf_accepted_rows.append({\n",
    "            \"new_id\": new_df.iloc[i][\"id\"],\n",
    "            \"new_query\": new_df.iloc[i][new_col],\n",
    "            \"resolved_id\": resolved_df.iloc[ridx][\"id\"],\n",
    "            \"resolved_query\": resolved_df.iloc[ridx][resolved_col],\n",
    "            \"method\": \"tfidf_char_wb_3_5\",\n",
    "            \"cosine_top1\": s1,\n",
    "            \"cosine_top2\": s2,\n",
    "            \"confidence_gap\": s1 - s2,\n",
    "            \"accepted_match\": bool(s1 >= tf_threshold),\n",
    "        })\n",
    "    tf_accept_path = os.path.join(out_dir, \"task1_tfidf_char_accepted.csv\")\n",
    "    pd.DataFrame(tf_accepted_rows).to_csv(tf_accept_path, index=False)\n",
    "    print(f\"=> Recommended TF-IDF cosine threshold ~= {tf_threshold:.2f} (0-1 scale)\")\n",
    "    print(f\"- Saved recommended TF-IDF accepted matches -> {tf_accept_path}\")\n",
    "\n",
    "    print(\"\\nNotes:\")\n",
    "    print(\"- For query matching, token_set_ratio is often robust to reordering and extra tokens.\")\n",
    "    print(\"- Char-level TF-IDF (3-5) tends to work well for short queries with typos/variants.\")\n",
    "    print(\"- The suggested thresholds are heuristic; adjust after spot-checking some pairs.\")\n",
    "\n",
    "\n",
    "def task2_match_names(\n",
    "    base_names_csv: str,\n",
    "    name_variations_csv: str,\n",
    "    out_dir: str,\n",
    "    topk: int = 3,\n",
    ") -> None:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(\"\\n=== Task 2: Matching names with variations ===\")\n",
    "\n",
    "    # Load data\n",
    "    base_df, base_col = load_csv_guess_text_column(base_names_csv, preferred_cols=[\"name\", \"full_name\"])\n",
    "    var_df, var_col = load_csv_guess_text_column(name_variations_csv, preferred_cols=[\"name\", \"full_name\"])\n",
    "    base_df = ensure_id_column(base_df, base_name=\"id\")\n",
    "    var_df = ensure_id_column(var_df, base_name=\"id\")\n",
    "\n",
    "    # Normalize names\n",
    "    base_df[\"_norm\"] = base_df[base_col].map(normalize_name)\n",
    "    var_df[\"_norm\"] = var_df[var_col].map(normalize_name)\n",
    "    # Also prepare token-sorted form to help token_sort/set ratios\n",
    "    base_df[\"_norm_sorted\"] = base_df[\"_norm\"].map(sorted_token_form)\n",
    "    var_df[\"_norm_sorted\"] = var_df[\"_norm\"].map(sorted_token_form)\n",
    "\n",
    "    base_norm = base_df[\"_norm\"].fillna(\"\").tolist()\n",
    "    var_norm = var_df[\"_norm\"].fillna(\"\").tolist()\n",
    "    base_norm_sorted = base_df[\"_norm_sorted\"].fillna(\"\").tolist()\n",
    "    var_norm_sorted = var_df[\"_norm_sorted\"].fillna(\"\").tolist()\n",
    "\n",
    "    # A. RapidFuzz on normalized and token-sorted forms\n",
    "    rf_results = {}\n",
    "    rf_stats = {}\n",
    "\n",
    "    # For names, token_set_ratio often performs best due to order invariance and duplicate handling\n",
    "    # We'll compute scores on the sorted token form to stabilize ratios\n",
    "    candidates = base_norm_sorted\n",
    "    queries = var_norm_sorted\n",
    "\n",
    "    for method in [\"token_set_ratio\", \"token_sort_ratio\", \"partial_ratio\", \"ratio\", \"WRatio\"]:\n",
    "        idx_mat, score_mat = rapidfuzz_topk(queries, candidates, method=method, topk=topk)\n",
    "        rf_results[method] = (idx_mat, score_mat)\n",
    "        rf_stats[method] = method_recommendation(score_mat)\n",
    "\n",
    "        # Save detailed CSV\n",
    "        rows = []\n",
    "        for i in range(len(var_df)):\n",
    "            for j in range(topk):\n",
    "                ridx = idx_mat[i, j]\n",
    "                score = score_mat[i, j]\n",
    "                if ridx < 0:\n",
    "                    continue\n",
    "                rows.append({\n",
    "                    \"variant_id\": var_df.iloc[i][\"id\"],\n",
    "                    \"variant_name\": var_df.iloc[i][var_col],\n",
    "                    \"base_id\": base_df.iloc[ridx][\"id\"],\n",
    "                    \"base_name\": base_df.iloc[ridx][base_col],\n",
    "                    \"method\": method,\n",
    "                    \"rank\": j + 1,\n",
    "                    \"score\": score,\n",
    "                })\n",
    "        out_path = os.path.join(out_dir, f\"task2_fuzzy_{method}.csv\")\n",
    "        pd.DataFrame(rows).to_csv(out_path, index=False)\n",
    "        print(f\"- Saved RapidFuzz name matches ({method}) -> {out_path}\")\n",
    "\n",
    "    method_scores = {m: rf_stats[m][\"median_gap\"] for m in rf_stats}\n",
    "    best_rf_method = max(method_scores, key=method_scores.get)\n",
    "    best_idx, best_score = rf_results[best_rf_method]\n",
    "    best_top1 = best_score[:, 0]\n",
    "    best_top2 = best_score[:, 1] if best_score.shape[1] > 1 else np.zeros_like(best_top1)\n",
    "    rf_threshold = suggest_threshold_from_gaps(best_top1, best_top2, hard_max=100.0, gap_margin=5.0)\n",
    "\n",
    "    print(\"\\nRapidFuzz method statistics (Task 2):\")\n",
    "    for m in rf_stats:\n",
    "        s = rf_stats[m]\n",
    "        print(f\"  {m:16s} median_top1={s['median_top1']:.1f} median_top2={s['median_top2']:.1f} median_gap={s['median_gap']:.1f}\")\n",
    "    print(f\"=> Recommended RapidFuzz method (names): {best_rf_method} with threshold ~= {rf_threshold:.1f} (0-100 scale)\")\n",
    "\n",
    "    # Consolidated accepted CSV for recommended method\n",
    "    accepted_rows = []\n",
    "    for i in range(len(var_df)):\n",
    "        ridx = best_idx[i, 0]\n",
    "        s1 = best_score[i, 0]\n",
    "        s2 = best_score[i, 1] if best_score.shape[1] > 1 else 0.0\n",
    "        accepted_rows.append({\n",
    "            \"variant_id\": var_df.iloc[i][\"id\"],\n",
    "            \"variant_name\": var_df.iloc[i][var_col],\n",
    "            \"base_id\": base_df.iloc[ridx][\"id\"] if ridx >= 0 else None,\n",
    "            \"base_name\": base_df.iloc[ridx][base_col] if ridx >= 0 else None,\n",
    "            \"method\": best_rf_method,\n",
    "            \"score_top1\": s1,\n",
    "            \"score_top2\": s2,\n",
    "            \"confidence_gap\": s1 - s2,\n",
    "            \"accepted_match\": bool(s1 >= rf_threshold),\n",
    "        })\n",
    "    rf_accept_path = os.path.join(out_dir, f\"task2_fuzzy_{best_rf_method}_accepted.csv\")\n",
    "    pd.DataFrame(accepted_rows).to_csv(rf_accept_path, index=False)\n",
    "    print(f\"- Saved recommended RF accepted name matches -> {rf_accept_path}\")\n",
    "\n",
    "    # B. TF-IDF for names: char n-grams (2,4) capture short tokens and initials\n",
    "    vec = fit_tfidf_vectorizer(\n",
    "        corpus_ref=base_norm,\n",
    "        analyzer=\"char_wb\",\n",
    "        ngram_range=(2, 4),\n",
    "        min_df=1,\n",
    "        max_df=1.0,\n",
    "    )\n",
    "    tf_idx, tf_sims = tfidf_topk(vec, var_norm, base_norm, topk=topk)\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(var_df)):\n",
    "        for j in range(tf_idx.shape[1]):\n",
    "            ridx = tf_idx[i, j]\n",
    "            sim = tf_sims[i, j]\n",
    "            rows.append({\n",
    "                \"variant_id\": var_df.iloc[i][\"id\"],\n",
    "                \"variant_name\": var_df.iloc[i][var_col],\n",
    "                \"base_id\": base_df.iloc[ridx][\"id\"],\n",
    "                \"base_name\": base_df.iloc[ridx][base_col],\n",
    "                \"method\": \"tfidf_char_wb_2_4\",\n",
    "                \"rank\": j + 1,\n",
    "                \"cosine_similarity\": sim,\n",
    "            })\n",
    "    tfidf_out = os.path.join(out_dir, \"task2_tfidf_char.csv\")\n",
    "    pd.DataFrame(rows).to_csv(tfidf_out, index=False)\n",
    "    print(f\"- Saved TF-IDF name matches -> {tfidf_out}\")\n",
    "\n",
    "    tf_top1 = tf_sims[:, 0]\n",
    "    tf_top2 = tf_sims[:, 1] if tf_sims.shape[1] > 1 else np.zeros_like(tf_top1)\n",
    "    tf_threshold = suggest_threshold_from_gaps(tf_top1, tf_top2, hard_max=1.0, gap_margin=0.05)\n",
    "\n",
    "    tf_accepted_rows = []\n",
    "    for i in range(len(var_df)):\n",
    "        ridx = tf_idx[i, 0]\n",
    "        s1 = tf_sims[i, 0]\n",
    "        s2 = tf_sims[i, 1] if tf_sims.shape[1] > 1 else 0.0\n",
    "        tf_accepted_rows.append({\n",
    "            \"variant_id\": var_df.iloc[i][\"id\"],\n",
    "            \"variant_name\": var_df.iloc[i][var_col],\n",
    "            \"base_id\": base_df.iloc[ridx][\"id\"],\n",
    "            \"base_name\": base_df.iloc[ridx][base_col],\n",
    "            \"method\": \"tfidf_char_wb_2_4\",\n",
    "            \"cosine_top1\": s1,\n",
    "            \"cosine_top2\": s2,\n",
    "            \"confidence_gap\": s1 - s2,\n",
    "            \"accepted_match\": bool(s1 >= tf_threshold),\n",
    "        })\n",
    "    tf_accept_path = os.path.join(out_dir, \"task2_tfidf_char_accepted.csv\")\n",
    "    pd.DataFrame(tf_accepted_rows).to_csv(tf_accept_path, index=False)\n",
    "    print(f\"=> Recommended TF-IDF cosine threshold (names) ~= {tf_threshold:.2f} (0-1 scale)\")\n",
    "    print(f\"- Saved recommended TF-IDF accepted name matches -> {tf_accept_path}\")\n",
    "\n",
    "    print(\"\\nNotes:\")\n",
    "    print(\"- For names, token_set_ratio on token-sorted normalized names is typically best.\")\n",
    "    print(\"- Char TF-IDF with (2,4) n-grams works well for initials and short tokens.\")\n",
    "    print(\"- Consider post-filters: same first letter, matching last token, or length ratio within 0.6-1.6 for higher precision.\")\n",
    "\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    p = argparse.ArgumentParser(description=\"Fuzzy and TF-IDF matching for queries and names\")\n",
    "    # Task 1\n",
    "    p.add_argument(\"--resolved_csv\", type=str, default=DEFAULT_RESOLVED_QUERIES, help=\"Resolved queries CSV (path or URL)\")\n",
    "    p.add_argument(\"--new_csv\", type=str, default=DEFAULT_NEW_QUERIES, help=\"New queries CSV (path or URL)\")\n",
    "    # Task 2\n",
    "    p.add_argument(\"--base_names_csv\", type=str, default=DEFAULT_BASE_NAMES, help=\"Base names CSV (path or URL)\")\n",
    "    p.add_argument(\"--name_variations_csv\", type=str, default=DEFAULT_NAME_VARIATIONS, help=\"Name variations CSV (path or URL)\")\n",
    "    # General\n",
    "    p.add_argument(\"--output_dir\", type=str, default=\"outputs_matching\", help=\"Directory to save outputs\")\n",
    "    p.add_argument(\"--topk\", type=int, default=3, help=\"Top-k matches to save per item\")\n",
    "    return p.parse_known_args()[0]\n",
    "\n",
    "\n",
    "def main():\n",
    "    args, unknown = parse_args(), None\n",
    "    print(\"Config:\")\n",
    "    print(f\"- Resolved CSV: {args.resolved_csv}\")\n",
    "    print(f\"- New CSV:      {args.new_csv}\")\n",
    "    print(f\"- Base names:   {args.base_names_csv}\")\n",
    "    print(f\"- Variations:   {args.name_variations_csv}\")\n",
    "    print(f\"- Output dir:   {args.output_dir}\")\n",
    "    print(f\"- Top-k:        {args.topk}\")\n",
    "\n",
    "    # Task 1\n",
    "    task1_dir = os.path.join(args.output_dir, \"task1\")\n",
    "    task1_match_queries(\n",
    "        resolved_csv=args.resolved_csv,\n",
    "        new_csv=args.new_csv,\n",
    "        out_dir=task1_dir,\n",
    "        topk=args.topk,\n",
    "    )\n",
    "\n",
    "    # Task 2\n",
    "    task2_dir = os.path.join(args.output_dir, \"task2\")\n",
    "    task2_match_names(\n",
    "        base_names_csv=args.base_names_csv,\n",
    "        name_variations_csv=args.name_variations_csv,\n",
    "        out_dir=task2_dir,\n",
    "        topk=args.topk,\n",
    "    )\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d3195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b88f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c7848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e70653f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5a144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
